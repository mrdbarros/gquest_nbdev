# AUTOGENERATED! DO NOT EDIT! File to edit: 01_decoder.ipynb (unless otherwise specified).

__all__ = ['data_path', 'train_data', 'test_data', 'sample_submission', 'target_columns', 'text_columns', 'splits',
           'df_tokenized,token_count', 'x_tfms', 'dbunch', '#learn', 'learn', 'learn', 'text_regression_learner']

# Cell
import pandas as pd # package for high-performance, easy-to-use data structures and data analysis
import numpy as np # fundamental package for scientific computing with Python

from fastai2.basics import *
from fastai2.text.all import *
from fastai2.callback.all import *
import pickle
import os
import torch
import numpy as np
from scipy.stats import spearmanr

# Cell
data_path = Path("/home/jupyter/mrdbarros/data/gquest_data/")

# Cell
print('Reading data...')
train_data = pd.read_csv(data_path/'train/train.csv')
test_data = pd.read_csv(data_path/'test/test.csv')
sample_submission = pd.read_csv(str(data_path/'sample_submission.csv'))
print('Reading data completed')

# Cell
train_data.columns
target_columns = train_data.columns[11:]
target_columns

# Cell
with open(data_path/'vocab.pkl', 'rb') as vocab_file:

    # Step 3
    lm_vocab = pickle.load(vocab_file)

    # After config_dictionary is read from file

# Cell
text_columns=['question_title', 'question_body', 'question_user_name',
       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',
       'url', 'category', 'host']


# Cell
splits = RandomSplitter()(train_data)

# Cell
df_tokenized,token_count=tokenize_df(train_data,text_columns)
x_tfms = [attrgetter("text"), Numericalize(vocab=lm_vocab)]


# Cell
torch.cuda.empty_cache()

# Cell
dbunch = dsrc.databunch(before_batch=pad_input,bs=16)


# Cell
#dbunch.show_batch(max_n=2,trunc_at=60)


# Cell
torch.cuda.empty_cache()

# Cell
#learn = text_classifier_learner(dbunch, AWD_LSTM, metrics=[accuracy], path=data_path,drop_mult=0.5)
learn = text_regression_learner(dbunch, AWD_LSTM,seq_len=300, path=data_path,drop_mult=0.5).to_fp16()

# Cell
learn = learn.load_encoder('enc1')


# Cell
len(target_columns)

# Cell
#pdb.set_trace()
learn.fit_one_cycle(4, moms=(0.8,0.7,0.8))

# Cell
@delegates(Learner.__init__)
def text_regression_learner(dbunch, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5,
                            lin_ftrs=None, ps=None, **kwargs):
    "Create a `Learner` with a text classifier from `data` and `arch`."
    vocab = _get_text_vocab(dbunch)
    model = get_text_classifier(arch, len(vocab), len(dbunch.train_ds[0][1]), seq_len=seq_len, config=config,
                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)
    meta = _model_meta[arch]
    learn = TextLearner(dbunch, model, loss_func=MSELossFlat(), splitter=meta['split_clas'], **kwargs)
    if pretrained:
        if 'url' not in meta:
            warn("There are no pretrained weights for that architecture yet!")
            return learn
        model_path = untar_data(meta['url'], c_key='model')
        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]
        learn = learn.load_pretrained(*fnames, model=learn.model[0])
        learn.freeze()
    return learn