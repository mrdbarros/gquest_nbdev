{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RoBERTa with fastai and HuggingFace ðŸ¤—Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of this great tutorial kernel and accompanying [article](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376): <br>\n",
    "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta\n",
    "\n",
    "Here, I've just implemented roBERTa, but go check out the original kernel to see how the same procedure could be used for BERT, RoBERTa, XLNet, XLM, and DistilBERT). I'd love if you upvote my kernel, but make sure to give the original votes, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Quest Q&A Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge is about questions and answers. \n",
    "\n",
    "In [question answering (QA)](https://en.wikipedia.org/wiki/Question_answering) systems are built that automatically answer questions posed by humans in a natural language. These computer systems excel at answering questions with single, verifiable answers. In contrast, humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context.  \n",
    "\n",
    "For the [Google QUEST Q&A Labeling competition](https://www.kaggle.com/c/google-quest-challenge/overview), we're tasked with predicting different subjective aspects of question-answering. The data for this competition includes questions and answers, and the task is to predict target values of 30 labels for each question-answer pair.Target labels with the prefix question_ relate to the question_title and/or question_body features in the data, and target labels with the prefix answer_ relate to the answer feature.\n",
    "\n",
    "This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Submissions are evaluated with the mean [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first **transfer learning** method applied to Natural Language Processing (NLP) was [Universal Language Model Fine-tuning for Text Classification](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf).(ULMFiT) method. This method involves starting with a pre-trained language model (LM), for example, trained on the Wikitext 103 dataset, and then fine tuning the language model on a new dataset. The fine tuned language model can then be used ina classification task for the new dataset. A demonstration is in the [fast.ai course](https://course.fast.ai/videos/?lesson=4), incorporating other techniques like discriminate learning rates, gradual model unfreezing, and slanted triangular learning rates.\n",
    "\n",
    "Recently, a new architecture called the **Transformer** (cf. [Attention is all you need](https://arxiv.org/abs/1706.03762)) has been shown to be powerful. Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) have pre-trained their own models (that use architectures based on the Transformer) on very large corpora. \n",
    "\n",
    "These transformers are availiable through the [HuggingFace](https://huggingface.co/) ðŸ¤— [transformers library](https://github.com/huggingface/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library has both pre-trained NLP models and additional utilities like tokenizers, optimizers and schedulers. \n",
    "\n",
    "This kernel uses the ``transformers`` library within the ``fastai`` framework. Specifically, I am using the [RoBERTa model](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8). I've broken the process down into different parts:\n",
    "1. Specifying Data Preprocessing\n",
    "1. Loading and Processing Data\n",
    "1. Creating the Model\n",
    "    - New! including the challenge metric\n",
    "1. Training the Model\n",
    "1. Predictions and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel uses fastai and Huggingface transformser. fastai is already installed on Kaggle, and [here](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/117716) is a discussion post that shows how to get Huggingface installled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path \n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random \n",
    "\n",
    "# fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "# classification metric\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig,AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement prints all of the directories in the /kaggle/input/ directory. This can be useful when trying to determine the path of the external datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utility function to set the seed for generating random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../input/google-quest-challenge')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"../input/google-quest-challenge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#os.chdir(Path(\"./gquest_nbdev\"))\n",
    "#os.chdir(Path(\"/home/mrdbarros/projetos/gquest_nbdev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 41) (476, 11)\n"
     ]
    }
   ],
   "source": [
    "model_type = 'roberta'\n",
    "pretrained_model_name = 'roberta-base' # 'roberta-base-openai-detector'\n",
    "DATA_ROOT = Path(\"../input/google-quest-challenge/\")\n",
    "MODEL_ROOT = Path(\"../input/\"+pretrained_model_name)\n",
    "train = pd.read_csv(DATA_ROOT / 'train.csv')\n",
    "test = pd.read_csv(DATA_ROOT / 'test.csv')\n",
    "sample_sub = pd.read_csv(DATA_ROOT / 'sample_submission.csv')\n",
    "print(train.shape,test.shape)\n",
    "download_model=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data. In this kernel, I'll use the `question_title`, `question_body` and `answer` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y Â  Â  e Â  Â  z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y Â  Â  e Â  Â  z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  ...  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
       "\n",
       "  question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0              1.000000        1.000000                     0.666667   \n",
       "1              0.888889        0.888889                     0.555556   \n",
       "2              0.777778        0.777778                     0.555556   \n",
       "3              0.888889        0.833333                     0.333333   \n",
       "4              1.000000        1.000000                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          1.000000          1.000000             0.800000   \n",
       "1          0.888889          0.888889             0.666667   \n",
       "2          1.000000          1.000000             0.666667   \n",
       "3          0.833333          1.000000             0.800000   \n",
       "4          1.000000          1.000000             0.800000   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                       1.0               0.000000   \n",
       "1                       0.0               0.000000   \n",
       "2                       0.0               0.333333   \n",
       "3                       0.0               0.000000   \n",
       "4                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted lables are in the columns of the sample submission. Note that some labels are with respect to the question, and some are with respect to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "labels = list(sample_sub.columns[1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding\n",
      "question_body_critical\n",
      "question_conversational\n",
      "question_expect_short_answer\n",
      "question_fact_seeking\n",
      "question_has_commonly_accepted_answer\n",
      "question_interestingness_others\n",
      "question_interestingness_self\n",
      "question_multi_intent\n",
      "question_not_really_a_question\n",
      "question_opinion_seeking\n",
      "question_type_choice\n",
      "question_type_compare\n",
      "question_type_consequence\n",
      "question_type_definition\n",
      "question_type_entity\n",
      "question_type_instructions\n",
      "question_type_procedure\n",
      "question_type_reason_explanation\n",
      "question_type_spelling\n",
      "question_well_written\n",
      "answer_helpful\n",
      "answer_level_of_information\n",
      "answer_plausible\n",
      "answer_relevance\n",
      "answer_satisfaction\n",
      "answer_type_instructions\n",
      "answer_type_procedure\n",
      "answer_type_reason_explanation\n",
      "answer_well_written\n"
     ]
    }
   ],
   "source": [
    "for label in labels: print(label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train[['question_title','question_body','answer']].to_csv(Path('../input/raw_text.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using pretrained models, the current data needs to be preprocessed in the same way as the data that trained the model. In ``transformers``, each model architecture is associated with 3 main types of classes:\n",
    "* A **model class** to load/store a particular pre-train model.\n",
    "* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n",
    "* A **configuration class** to load/store the configuration of a particular model.\n",
    "\n",
    "For the RoBERTa architecture, we use `RobertaForSequenceClassification` for the **model class**, `RobertaTokenizer` for the **tokenizer class**, and `RobertaConfig` for the **configuration class**.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'albert': (AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, \n",
    "                RobertaConfig(hidden_act=\"gelu_new\",\n",
    "                              hidden_dropout_prob=0.1,\n",
    "                              attention_probs_dropout_prob=0.1,\n",
    "                              #max_position_embeddings=1024,\n",
    "                              layer_norm_eps=1e-12))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,Â ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 42\n",
    "use_fp16 = True\n",
    "bs = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜../input/roberta-baseâ€™: File exists\r\n"
     ]
    }
   ],
   "source": [
    "if download_model:\n",
    "    new_dir=Path(\"../input\")/pretrained_model_name\n",
    "    !mkdir {new_dir}\n",
    "    transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "    transformer_tokenizer.save_pretrained(MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class.pretrained_model_archive_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the RoBERTa tokenizer and numericalizer in fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is preprocessed through tokenization and numericalization. To match the pretrained models, we need to use the same tokenization and numericalization as the model. Fortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n",
    "\n",
    "In ``fastai``, data pre-processing is performed during the creation of the ``DataBunch``. When creating a `DataBunch`, the tokenizer and numericalizer are passed in the processor argument.\n",
    "\n",
    "Therefore, the first step is to create a customized tokenize and numericalizer that use the correct transformer tokenizer classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokentizer takes the text and transforms it into tokens. The ``fastai`` documentation notes that: \n",
    "1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n",
    "2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n",
    "3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) â†’ List[str]`` that take a text ``t`` and returns the list of its tokens.\n",
    "\n",
    "To use the RoBERTa tokenizer, we create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function. It is important to note that RoBERTa requires a space to start the input string. The encoding methods should be called with ``add_prefix_space`` set to ``True``. The output of the tokenizer should have the following pattern. (Note that padding is added when the `DataBunch` is created.)\n",
    "\n",
    "    roberta: [CLS] + prefix_space + tokens + [SEP] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'albert', **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "        return [CLS] + tokens + [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if download_model:\n",
    "    transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "    transformer_tokenizer.save_pretrained(MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transformer_tokenizer = tokenizer_class.from_pretrained(MODEL_ROOT)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Numericalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numericalizer takes the the tokens, and turns them into numbers. The ``fastai`` documentation notes that:\n",
    "1. The [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab)\n",
    "\n",
    "To use the RoBERTa numericalizer, we create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
    "\n",
    "    def __setstate__(self, state:dict):\n",
    "        self.itos = state['itos']\n",
    "        self.tokenizer = state['tokenizer']\n",
    "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
    "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
    "\n",
    "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "transformer_processor = [tokenize_processor, numericalize_processor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a custom processor, which contains the custom tokenizer and numericalizer, we can create the `DataBunch`. During the DataBunch creation, we have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding. For RoBERTa, it's usually advised to pad the inputs on the right rather than the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pad_first = bool(model_type in ['xlnet'])\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel uses [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), to create the `DataBunch`. \n",
    "\n",
    "In the `DataBunch` creation, I have specified to use the 'question_title','question_body', and 'answer' columns as the training data. Recall from the introduction that some of the target answers relate to the question (title + body) and some only to the answer. It's an open question as to whether it's a good choice to stick these all together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.666667]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ]),\n",
       " array([0.2     , 0.266667, 0.3     , 0.333333, 0.4     , 0.466667, 0.5     , 0.533333, 0.6     , 0.666667, 0.7     ,\n",
       "        0.733333, 0.8     , 0.866667, 0.9     , 0.933333, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.      , 0.333333, 0.5     , 0.666667, 1.      ]),\n",
       " array([0.333333, 0.444444, 0.5     , 0.555556, 0.666667, 0.777778, 0.833333, 0.888889, 1.      ])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sorted_values=[np.sort(train[labels[i]].unique()) for i in range(len(labels))]\n",
    "unique_sorted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "databunch = (TextList.from_df(train, cols=['question_title','question_body','answer'], processor=transformer_processor)\n",
    "             .split_subsets(train_size=0.05,valid_size=0.05)\n",
    "             .label_from_df(cols=labels)\n",
    "             .add_test(test)\n",
    "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check batch and tokenizer. Because the RoBERTa tokenizer was used, there are a lot of 'G' in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ä event Ä hor izons Ä are Ä unt ra vers able Ä by Ä observers Ä far Ä from Ä the Ä collapse ? Ä Consider Ä this Ä a Ä follow up Ä question Ä of Ä this Ä one ÄŠ ÄŠ In Ä the Ä classical Ä sch w ars z child Ä solution Ä with Ä an Ä eternal Ä black Ä hole , Ä the Ä user Ä falls Ä through Ä the Ä event Ä horizon Ä in Ä finite Ä local Ä time , Ä but Ä this Ä event Ä does Ä take Ä place Ä for Ä distant Ä observers Ä in Ä the Ä infinite Ä future . Ä As Ä Leonard</td>\n",
       "      <td>question_expect_short_answer;question_fact_seeking;question_has_commonly_accepted_answer;question_multi_intent;question_type_choice;question_type_reason_explanation;question_well_written;answer_helpful;answer_level_of_information;answer_plausible;answer_relevance;answer_type_reason_explanation;answer_well_written</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ä How Ä are Ä pixels Ä actually Ä shown Ä on Ä display Ä I Ä spend Ä a Ä lot Ä of Ä time Ä looking Ä at Ä raw - video Ä in Ä Y C b Cr - format . ÄŠ Start ed Ä thinking Ä about Ä what Ä is Ä really Ä shown Ä on Ä the Ä display Ä and Ä how ÄŠ the Ä pixels Ä end Ä up Ä there . ÄŠ ÄŠ No Ä display Ä can Ä handle Ä Y C b Cr - data Ä so Ä I Ä guess Ä everything Ä gets Ä converted ÄŠ to Ä RGB Ä somewhere</td>\n",
       "      <td>question_asker_intent_understanding;question_fact_seeking;question_has_commonly_accepted_answer;question_type_choice;question_type_reason_explanation;answer_plausible;answer_relevance;answer_type_reason_explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ä Tags Ä to Ä Post - ID Ä mysql Ä query . Ä Tag Ä Search Ä Im Ä trying Ä to Ä implement Ä a Ä tag - based Ä search . Ä When Ä i Ä specify Ä certain Ä tags , Ä tagged Ä posts Ä will Ä be Ä searched Ä and Ä the Ä post - id Ä will Ä be Ä displayed Ä that Ä matches Ä the Ä search crit eria . ÄŠ ÄŠ Currently Ä it Ä only Ä works Ä for Ä a Ä single Ä tag . ÄŠ ÄŠ $ query Ä = Ä \" SELECT Ä D IST IN CT Ä $</td>\n",
       "      <td>question_has_commonly_accepted_answer;question_type_instructions;answer_helpful;answer_plausible;answer_relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ä How Ä can Ä I Ä get Ä to Ä view Ä the Ä North Ä Face Ä of Ä the Ä E iger Ä in Ä Switzerland ? Ä I Ä can 't Ä seem Ä to Ä find Ä information Ä online Ä about Ä trains , Ä buses , Ä to Ä take Ä to Ä get Ä there Ä etc . Ä I Ä would Ä be Ä travelling Ä in Ä from Ä Stain ach Ä in Ä Austria . ÄŠ ÄŠ Is Ä there Ä a Ä mountain Ä station Ä or Ä similar Ä that Ä I Ä can Ä view Ä it Ä from ? ÄŠ ÄŠ e : Ä I Ä see</td>\n",
       "      <td>question_asker_intent_understanding;question_expect_short_answer;question_fact_seeking;question_multi_intent;question_type_instructions;answer_helpful;answer_plausible;answer_type_instructions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ä . x ls x Ä and Ä x ls ( Latest Ä Vers ions ) Ä to Ä pdf Ä using Ä python Ä With Ä the Ä help Ä of Ä this Ä . doc Ä to Ä pdf Ä using Ä python ÄŠ Link Ä I Ä am Ä trying Ä for Ä excel Ä (. x ls x Ä and Ä x ls Ä formats ) ÄŠ ÄŠ Following Ä is Ä modified Ä Code Ä for Ä Excel : ÄŠ ÄŠ import Ä os ÄŠ from Ä win 32 com Ä import Ä client ÄŠ ÄŠ folder Ä = Ä \" C</td>\n",
       "      <td>question_expect_short_answer;question_fact_seeking;question_has_commonly_accepted_answer;question_type_instructions;answer_relevance;answer_type_instructions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print('[CLS] token :', transformer_tokenizer.cls_token)\n",
    "#print('[SEP] token :', transformer_tokenizer.sep_token)\n",
    "#print('[PAD] token :', transformer_tokenizer.pad_token)\n",
    "databunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check batch and numericalizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    0, 48042,    12,  ..., 45803, 45149,     2],\n",
      "        [    0,  1336,     7,  ...,     6,     8,     2],\n",
      "        [    0,   515, 17211,  ...,  8287, 17604,     2],\n",
      "        ...,\n",
      "        [    0,   256, 14858,  ...,  1437,  1437,     2],\n",
      "        [    0,   653,    18,  ...,  1437,  1437,     2],\n",
      "        [    0, 19267,   330,  ...,    11,  5588,     2]]), tensor([[0.6667, 0.3333, 0.0000, 0.0000, 1.0000, 1.0000, 0.3333, 0.3333, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000,\n",
      "         0.0000, 0.0000, 0.6667, 0.6667, 0.3333, 0.6667, 0.6667, 0.6000, 0.0000,\n",
      "         0.5000, 0.0000, 0.8333],\n",
      "        [1.0000, 0.7778, 0.0000, 0.6667, 0.6667, 0.6667, 0.4444, 0.5556, 0.3333,\n",
      "         0.0000, 1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.3333,\n",
      "         0.0000, 0.0000, 0.8889, 1.0000, 0.7778, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.3333, 0.0000, 0.8889],\n",
      "        [0.6667, 0.3333, 0.0000, 1.0000, 1.0000, 1.0000, 0.6667, 0.6667, 1.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8000, 0.0000,\n",
      "         0.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.5556, 0.0000, 0.0000, 0.0000, 0.0000, 0.5556, 0.8889, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.3333,\n",
      "         0.3333, 0.0000, 1.0000, 1.0000, 0.7778, 1.0000, 1.0000, 1.0000, 0.3333,\n",
      "         0.3333, 1.0000, 1.0000],\n",
      "        [1.0000, 0.8889, 1.0000, 0.0000, 0.0000, 0.0000, 0.7778, 0.7778, 0.0000,\n",
      "         0.6667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "         0.3333, 0.0000, 1.0000, 0.8889, 0.5556, 1.0000, 1.0000, 0.8667, 0.0000,\n",
      "         0.0000, 0.6667, 1.0000],\n",
      "        [0.7778, 0.4444, 0.0000, 1.0000, 1.0000, 1.0000, 0.6667, 0.4444, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333,\n",
      "         0.6667, 0.0000, 0.7778, 1.0000, 0.6667, 1.0000, 1.0000, 0.9000, 0.6667,\n",
      "         0.3333, 0.0000, 0.6667],\n",
      "        [0.8889, 0.3333, 0.0000, 1.0000, 1.0000, 1.0000, 0.5556, 0.4444, 0.6667,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6667, 0.0000, 0.7778, 1.0000, 0.7778, 1.0000, 1.0000, 1.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.8889],\n",
      "        [0.8889, 0.3333, 0.0000, 1.0000, 0.3333, 1.0000, 0.5556, 0.3333, 0.0000,\n",
      "         0.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.3333, 1.0000, 0.6667, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.0000, 0.0000, 1.0000]]))\n"
     ]
    }
   ],
   "source": [
    "#print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
    "#print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
    "#print('[PAD] id :', pad_idx)\n",
    "test_one_batch = databunch.one_batch()\n",
    "#print('Batch shape : ',test_one_batch.shape)\n",
    "print(test_one_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), the RoBERTa model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits. One way to access them is to create a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# defining our model architecture \n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "            \n",
    "        logits = self.transformer(input_ids,\n",
    "                                attention_mask = attention_mask)[0]   \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the transformer adapted to multiclass classification, we need to specify the number of labels before loading the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if download_model:\n",
    "    config = config_class.from_pretrained(pretrained_model_name)\n",
    "    config.save_pretrained(MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "config = config_class.from_pretrained(MODEL_ROOT,num_labels=30)\n",
    "config.use_bfloat16 = use_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if download_model:\n",
    "    transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
    "    transformer_model.save_pretrained(MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transformer_model = model_class.from_pretrained(MODEL_ROOT, config = config)\n",
    "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to be able to see how well the model is doing. For this competition: \n",
    "> Submissions are evaluated on the mean column-wise Spearman's correlation coefficient. The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.\n",
    "\n",
    "Although scipy provides an implementation of [Spearman's R](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.stats.spearmanr.html), we also need to take the average across all of the columns. Therefore,  we need to create our own [custom metric](https://docs.fast.ai/metrics.html#Creating-your-own-metric). The custom metric is only used on the validations set.\n",
    "- `on_epoch_begin`: create empty numpy arrays to hold the predictions and targets\n",
    "- `on_batch_end`: after each back, append the most recent output (predictions) and targets\n",
    "- `on_epoch_end`: when the epoch is finished, compute Spearman's R on the columns, and then take the average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "class AvgSpearman(Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.preds = np.empty( shape=(0, 30) )\n",
    "        self.target = np.empty( shape=(0, 30) )\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        self.preds = np.append(self.preds,last_output.detach().cpu(),axis=0)\n",
    "        self.target = np.append(self.target,last_target.detach().cpu(),axis=0)\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        spearsum = 0\n",
    "        for col in range(self.preds.shape[1]):\n",
    "            spearsum += spearmanr(self.preds[:,col],self.target[:,col]).correlation\n",
    "        res = spearsum / (self.preds.shape[1] + 1)\n",
    "        return add_metrics(last_metrics, res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai LearnerÂ with Custom Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fastai, the `Learner` holds the data, model and other parameter, like the optimizer. Since we're using transformers, we want to use an optimizer designed for them: the AdamW optimizer. This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``. To reproduce BertAdam specific behavior, you have to set ``correct_bias = False``. We include our new AvgSpearman metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5262, 0.1913, 0.1052, 0.0929, 0.0585, 0.0126, 0.0072, 0.0040, 0.0022],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor(train[labels[0]].value_counts(normalize=True).sort_values().values,dtype=torch.float32).cuda()\n",
    "weights=(1/weights)/(1/weights).sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FlattenedLoss_BWW(FlattenedLoss):\n",
    "    def __init__(self,unique_sorted_values,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.unique_sorted_values=unique_sorted_values\n",
    "        \n",
    "    \n",
    "    def __call__(self, input:Tensor, target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        \n",
    "        input = input.transpose(self.axis,-1).contiguous()\n",
    "        target = target.transpose(self.axis,-1).contiguous()\n",
    "        if self.floatify: target = target.float()\n",
    "        input = input.view(-1,input.shape[-1]) if self.is_2d else input.view(-1)\n",
    "        total_entropy=torch.tensor(0.0).cuda()\n",
    "        pos = 0\n",
    "        for i in range(len(self.unique_sorted_values)):\n",
    "            labeled_target=torch.empty(target.shape[0],dtype=torch.long).cuda()\n",
    "            for j in range(len(self.unique_sorted_values[i])):\n",
    "                labeled_target[(target[:,i]== self.unique_sorted_values[i][j]).nonzero(as_tuple=True)] = j\n",
    "                if j==0:\n",
    "                    occurences = (target[:,i] == self.unique_sorted_values[i][j]).sum(dtype=torch.float).unsqueeze(dim=0)\n",
    "                else:\n",
    "                    occurences = torch.cat((occurences,(target[:,i] == self.unique_sorted_values[i][j]).sum(dtype=torch.float).unsqueeze(dim=0)),axis=0)\n",
    "            new_weights=torch.where(occurences>0.,1/occurences,torch.zeros(occurences.shape).cuda())\n",
    "            new_weights = new_weights / new_weights.sum()\n",
    "            self.func.weight = new_weights\n",
    "            #pdb.set_trace()\n",
    "            total_entropy+=self.func.__call__(input[:,pos:(pos+len(self.unique_sorted_values[i]))], \n",
    "                                              labeled_target, **kwargs)\n",
    "            pos+=len(self.unique_sorted_values[i])\n",
    "        return total_entropy/len(self.unique_sorted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def CrossEntropyFlat_BWW(unique_sorted_values,*args, axis:int=-1, **kwargs):\n",
    "    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n",
    "    return_loss=FlattenedLoss_BWW(unique_sorted_values,nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n",
    "    return return_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from fastai.callbacks import *\n",
    "from transformers import AdamW\n",
    "\n",
    "learner = Learner(databunch, \n",
    "                  custom_transformer_model, \n",
    "                  opt_func = lambda input: AdamW(input,correct_bias=False), \n",
    "                  metrics=[AvgSpearman()])\n",
    "\n",
    "# Show graph of learner stats and metrics after each epoch.\n",
    "learner.callbacks.append(ShowGraph(learner))\n",
    "\n",
    "# Put learn in FP16 precision mode. --> Not working in the tutorial\n",
    "if use_fp16: learner = learner.to_fp16()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the Learner, we can train the model. During training, we are going to use techniques known to help in other classification tasks: **discriminative layer training**, **gradual unfreezing** and **slanted triangular learning rates**. The kernel tutorial author noted that he didn't find any documentation about influence of these techniques with transformers. I've used them because I think that these techniques are probably domain general, and will therefore give a boost in this system. \n",
    "\n",
    "To implement unfreezing, our model needs to be specified into different layer groups. ``fastai`` allows us to \"split\" the structure model into groups, [described here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training).\n",
    "\n",
    "To see the structure of the RoBERTa model, look at the output of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformerModel(\n",
      "  (transformer): RobertaForSequenceClassification(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (classifier): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (out_proj): Linear(in_features=768, out_features=30, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(learner.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many layer groups we currently have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner split in 1 groups\n"
     ]
    }
   ],
   "source": [
    "num_groups = len(learner.layer_groups)\n",
    "print('Learner split in',num_groups,'groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#learner.model.transformer.albert.encoder.albert_layer_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One group won't allow us to unfreeze parts of the model. The tutorial kernel suggested to divide the RoBERTa model in 14 blocks:\n",
    "* 1 Embedding\n",
    "* 12 transformer\n",
    "* 1 classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_layers = [learner.model.transformer.roberta.embeddings,\n",
    "              learner.model.transformer.roberta.encoder.layer[0],\n",
    "              learner.model.transformer.roberta.encoder.layer[1],\n",
    "              learner.model.transformer.roberta.encoder.layer[2],\n",
    "              learner.model.transformer.roberta.encoder.layer[3],\n",
    "              learner.model.transformer.roberta.encoder.layer[4],\n",
    "              learner.model.transformer.roberta.encoder.layer[5],\n",
    "              learner.model.transformer.roberta.encoder.layer[6],\n",
    "              learner.model.transformer.roberta.encoder.layer[7],\n",
    "              learner.model.transformer.roberta.encoder.layer[8],\n",
    "              learner.model.transformer.roberta.encoder.layer[9],\n",
    "              learner.model.transformer.roberta.encoder.layer[10],\n",
    "              learner.model.transformer.roberta.encoder.layer[11],\n",
    "              learner.model.transformer.roberta.pooler]\n",
    "\n",
    "learner.split(list_layers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#list_layers = [learner.model.transformer.albert.embeddings,\n",
    "#              learner.model.transformer.albert.encoder.albert_layer_groups[0],\n",
    "#              learner.model.transformer.albert.pooler]\n",
    "\n",
    "list_layers = [learner.model.transformer.roberta.embeddings,\n",
    "              learner.model.transformer.roberta.encoder.layer[0],\n",
    "              learner.model.transformer.roberta.encoder.layer[1],\n",
    "              learner.model.transformer.roberta.encoder.layer[2],\n",
    "              learner.model.transformer.roberta.encoder.layer[3],\n",
    "              learner.model.transformer.roberta.encoder.layer[4],\n",
    "              learner.model.transformer.roberta.encoder.layer[5],\n",
    "              learner.model.transformer.roberta.encoder.layer[6],\n",
    "              learner.model.transformer.roberta.encoder.layer[7],\n",
    "              learner.model.transformer.roberta.encoder.layer[8],\n",
    "              learner.model.transformer.roberta.encoder.layer[9],\n",
    "              learner.model.transformer.roberta.encoder.layer[10],\n",
    "              learner.model.transformer.roberta.encoder.layer[11],\n",
    "              learner.model.transformer.roberta.pooler]\n",
    "\n",
    "learner.split(list_layers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we now have 14 layer groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner split in 14 groups\n"
     ]
    }
   ],
   "source": [
    "num_groups = len(learner.layer_groups)\n",
    "print('Learner split in',num_groups,'groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we will:\n",
    "1. Find an appropriate initial learning rate\n",
    "1. Progressively unfreeze the layers while training\n",
    "\n",
    "During all training, we use the **Slanted Triangular Learning Rates** with the `.fit_one_cycle` command, described [here](https://docs.fast.ai/callbacks.one_cycle.html). Originally, I wanted to unfreeze the entire model, but I kept running out of space. I'll trouble shoot in other versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find an appropriate learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to randomness, there can be little differences in the learning rate. Based on a few runs on my computer, I've chosen 2e-4 for the starting point of my kaggle submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train with progressive unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the code to progressively unfreeze and train the model is very repetitive, I have made it into a loop. I unfreeze only the first 5 layer groups because I run out of memory after that. The learning rates and number of epochs are mostly arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "unfreeze_layers = [-1,-2,-3]\n",
    "learning_rates = [3e-4, 1e-5, 5e-6]\n",
    "epochs = [3,4,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfreeze_layers = [-1,-5,-9,-14]\n",
    "learning_rates = [2e-4, 5e-5,  5e-5, 1e-5]\n",
    "epochs = [2, 2, 3,4]\n",
    "learner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def model_unfreezing_and_training():\n",
    "    for layer in range(0,1):\n",
    "        print(layer)\n",
    "        if layer == num_groups-1: \n",
    "            learner.unfreeze()     \n",
    "        else: \n",
    "            learner.freeze_to(unfreeze_layers[layer])\n",
    "        \n",
    "        print('freezing to:',unfreeze_layers[layer],' - ',epochs[layer],'epochs')\n",
    "        learner.fit_one_cycle(epochs[layer], \n",
    "                              max_lr=slice(learning_rates[layer]*0.95**num_groups, learning_rates[layer]),\n",
    "                              moms=(0.8, 0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "freezing to: -1  -  2 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 00:32<00:32]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>avg_spearman</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.495148</td>\n",
       "      <td>0.432452</td>\n",
       "      <td>0.124361</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='11' class='' max='37', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      29.73% [11/37 00:06<00:14 0.4755]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a30c4c50f8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_unfreezing_and_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#bce loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-15b39c46c06e>\u001b[0m in \u001b[0;36mmodel_unfreezing_and_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         learner.fit_one_cycle(epochs[layer], \n\u001b[1;32m     11\u001b[0m                               \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                               moms=(0.8, 0.9))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m\"Handle gradient calculation on `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_unfreezing_and_training() #bce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_unfreezing_and_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_groups\n",
    "0.95**num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rates[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "slice(learning_rates[2]*0.95**num_groups, learning_rates[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "learner.fit_one_cycle(10, \n",
    "                              max_lr=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(MODEL_ROOT.resolve()/\"export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=load_learner(MODEL_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "learner.fit_one_cycle(5, \n",
    "                              max_lr=slice(learning_rates[2]*0.95**num_groups, learning_rates[2]),\n",
    "                              moms=(0.8, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can generate our predictions from the test dataset. As [noted in other tutorials](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) the function ``get_preds`` does not return elements in order by default. Therefore, we will have to resort the test elements into their correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='30', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/30 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 3.94 GiB total capacity; 3.11 GiB already allocated; 14.81 MiB free; 3.12 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ed1051990e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds_as_nparray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-ed1051990e3d>\u001b[0m in \u001b[0;36mget_preds_as_nparray\u001b[0;34m(ds_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwe\u001b[0m \u001b[0mborrow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mRNNLearner\u001b[0m \u001b[0mto\u001b[0m \u001b[0mresort\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melements\u001b[0m \u001b[0minto\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mreverse_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_type, activ, with_loss, n_batch, pbar)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         return get_preds(self.model, self.dl(ds_type), cb_handler=CallbackHandler(callbacks),\n\u001b[0;32m--> 341\u001b[0;31m                          activ=activ, loss_func=lf, n_batch=n_batch, pbar=pbar)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     def pred_batch(self, ds_type:DatasetType=DatasetType.Valid, batch:Tuple=None, reconstruct:bool=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, dl, pbar, cb_handler, activ, loss_func, n_batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     res = [to_float(torch.cat(o).cpu()) for o in\n\u001b[0;32m---> 44\u001b[0;31m            zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mNoneReduceOnCPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-fe5b2839e854>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         logits = self.transformer(input_ids,\n\u001b[0;32m---> 10\u001b[0;31m                                 attention_mask = attention_mask)[0]   \n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         )\n\u001b[1;32m    389\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         )\n\u001b[1;32m    813\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             )\n\u001b[1;32m    424\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     ):\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[1;32m    328\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         )\n\u001b[1;32m    331\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 3.94 GiB total capacity; 3.11 GiB already allocated; 14.81 MiB free; 3.12 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the get_preds method does not yield the elements in order by default\n",
    "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
    "    \"\"\"\n",
    "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    return preds[reverse_sampler, :]\n",
    "\n",
    "test_preds = get_preds_as_nparray(DatasetType.Test).clip(0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(10,1,-1)\n",
    "labels=np.random.randn(9)\n",
    "labels[a.astype(int)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.080917, -0.303564, -0.488476, -0.080917,  0.210601, -0.488476, -0.303564, -0.488476])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds,test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds,test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(DATA_ROOT / 'sample_submission.csv')\n",
    "sample_submission[labels] = test_preds\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for looking through this kernel! I hope that it helps you understand transformers, and how to integrate Huggingface with fastai. \n",
    "\n",
    "Check out the original for some other cool architectures:\n",
    "[Fastai with HuggingFace ðŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Fastai with HuggingFace ðŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta)\n",
    "* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
    "* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n",
    "* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n",
    "* Keita Kurita's articleÂ : [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)Â (May 2019)\n",
    "* Dev Sharma's articleÂ : [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
